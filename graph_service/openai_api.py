"""
OpenAIå…¼å®¹çš„APIæ¥å£
ç”¨äºé›†æˆOpenWebUI
"""
from fastapi import APIRouter
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any, AsyncIterator
from loguru import logger
import time
import json

from .graph import compile_graph
from .state import GraphState
from .utils import smart_truncate, get_tool_type, extract_result_summary


router = APIRouter()

# ç¼–è¯‘å›¾(å¤ç”¨main.pyä¸­çš„)
graph = None


def get_graph():
    """è·å–æˆ–åˆ›å»ºå›¾å®ä¾‹"""
    global graph
    if graph is None:
        graph = compile_graph()
    return graph


class Message(BaseModel):
    """æ¶ˆæ¯æ¨¡å‹"""
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    """OpenAIèŠå¤©è¡¥å…¨è¯·æ±‚"""
    model: str
    messages: List[Message]
    stream: Optional[bool] = False
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2000


class ChatCompletionResponse(BaseModel):
    """OpenAIèŠå¤©è¡¥å…¨å“åº”"""
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]


@router.get("/v1/models")
async def list_models():
    """
    åˆ—å‡ºå¯ç”¨æ¨¡å‹
    OpenAIå…¼å®¹æ¥å£
    """
    return {
        "object": "list",
        "data": [
            {
                "id": "aiagent-network-tools",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "aiagent",
                "permission": [],
                "root": "aiagent-network-tools",
                "parent": None,
            }
        ]
    }


@router.get("/v1/models/{model_id}")
async def get_model(model_id: str):
    """
    è·å–å•ä¸ªæ¨¡å‹ä¿¡æ¯
    OpenAIå…¼å®¹æ¥å£
    """
    if model_id == "aiagent-network-tools":
        return {
            "id": "aiagent-network-tools",
            "object": "model",
            "created": int(time.time()),
            "owned_by": "aiagent",
            "permission": [],
            "root": "aiagent-network-tools",
            "parent": None,
        }
    else:
        from fastapi import HTTPException
        raise HTTPException(status_code=404, detail=f"Model '{model_id}' not found")


@router.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """
    èŠå¤©è¡¥å…¨æ¥å£
    OpenAIå…¼å®¹æ¥å£
    """
    try:
        # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_message = ""
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_message = msg.content
                break

        if not user_message:
            user_message = request.messages[-1].content if request.messages else ""

        logger.info(f"OpenAI APIæ”¶åˆ°è¯·æ±‚: {user_message[:100]}...")

        # åˆå§‹åŒ–çŠ¶æ€
        initial_state: GraphState = {
            "user_query": user_message,
            "current_node": "",
            "target_agent": "",
            "network_diag_result": None,
            "rag_result": None,
            "final_answer": "",
            "errors": [],
            "metadata": {}
        }

        # æ‰§è¡Œå›¾
        graph_instance = get_graph()

        if request.stream:
            # æµå¼å“åº” - ä½¿ç”¨ astream() å®æ—¶è¿”å›
            logger.info("ä½¿ç”¨æµå¼æ¨¡å¼æ‰§è¡Œå›¾")
            return StreamingResponse(
                _stream_response(graph_instance, initial_state, request.model),
                media_type="text/event-stream"
            )
        else:
            # éæµå¼å“åº” - ä½¿ç”¨ ainvoke() ç­‰å¾…å®Œæˆ
            logger.info("ä½¿ç”¨éæµå¼æ¨¡å¼æ‰§è¡Œå›¾")
            final_state = await graph_instance.ainvoke(
                initial_state,
                config={"recursion_limit": 100}  # å¢åŠ é€’å½’é™åˆ¶åˆ° 100ï¼Œæ”¯æŒå¤š Agent ä¸²è¡Œæ‰§è¡Œ
            )

            # æ„å»ºå“åº”
            response_text = final_state["final_answer"]

            logger.info(f"OpenAI APIå‡†å¤‡è¿”å›å“åº”,é•¿åº¦: {len(response_text)} å­—ç¬¦")
            logger.debug(f"å“åº”å†…å®¹: {response_text[:200]}...")

            response_data = {
                "id": f"chatcmpl-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": request.model,
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": response_text
                        },
                        "finish_reason": "stop"
                    }
                ],
                "usage": {
                    "prompt_tokens": len(user_message.split()),
                    "completion_tokens": len(response_text.split()),
                    "total_tokens": len(user_message.split()) + len(response_text.split())
                }
            }
            logger.info("OpenAI APIå“åº”å·²æ„å»º,å‡†å¤‡è¿”å›")
            return JSONResponse(content=response_data)

    except Exception as e:
        logger.error(f"OpenAI APIå¤„ç†å¤±è´¥: {e}")
        error_response = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": f"å¤„ç†å¤±è´¥: {str(e)}"
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        }
        return JSONResponse(content=error_response)


async def _stream_response(graph, initial_state: GraphState, model: str) -> AsyncIterator[str]:
    """
    ç”Ÿæˆæµå¼å“åº”

    Args:
        graph: LangGraph å›¾å®ä¾‹
        initial_state: åˆå§‹çŠ¶æ€
        model: æ¨¡å‹åç§°

    Yields:
        SSEæ ¼å¼çš„æ•°æ®å—
    """
    try:
        chat_id = f"chatcmpl-{int(time.time())}"
        created_time = int(time.time())

        # ç”¨äºç´¯ç§¯æœ€ç»ˆç­”æ¡ˆ
        accumulated_content = ""

        # ä½¿ç”¨ astream() æµå¼æ‰§è¡Œå›¾
        async for chunk in graph.astream(
            initial_state,
            stream_mode="updates",  # è·å–çŠ¶æ€æ›´æ–°
            config={"recursion_limit": 100}
        ):
            # chunk æ ¼å¼: {node_name: state_update}
            for node_name, state_update in chunk.items():
                logger.info(f"æµå¼è¾“å‡º - èŠ‚ç‚¹: {node_name}, æ›´æ–°: {list(state_update.keys())}")
                logger.debug(f"æµå¼è¾“å‡º - å®Œæ•´æ›´æ–°: {state_update}")

                # æ ¼å¼åŒ–èŠ‚ç‚¹è¾“å‡º
                content = _format_node_output(node_name, state_update)

                if content:
                    accumulated_content += content

                    # å‘é€å†…å®¹å—
                    response_chunk = {
                        "id": chat_id,
                        "object": "chat.completion.chunk",
                        "created": created_time,
                        "model": model,
                        "choices": [
                            {
                                "index": 0,
                                "delta": {
                                    "content": content
                                },
                                "finish_reason": None
                            }
                        ]
                    }

                    yield f"data: {json.dumps(response_chunk, ensure_ascii=False)}\n\n"

        # å‘é€ç»“æŸæ ‡è®°
        end_chunk = {
            "id": chat_id,
            "object": "chat.completion.chunk",
            "created": created_time,
            "model": model,
            "choices": [
                {
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }
            ]
        }

        yield f"data: {json.dumps(end_chunk)}\n\n"
        yield "data: [DONE]\n\n"

        logger.info(f"æµå¼å“åº”å®Œæˆï¼Œæ€»é•¿åº¦: {len(accumulated_content)} å­—ç¬¦")

    except Exception as e:
        logger.error(f"æµå¼å“åº”ç”Ÿæˆå¤±è´¥: {e}")
        # å‘é€é”™è¯¯ä¿¡æ¯
        error_chunk = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [
                {
                    "index": 0,
                    "delta": {
                        "content": f"\n\nâŒ é”™è¯¯: {str(e)}\n"
                    },
                    "finish_reason": "stop"
                }
            ]
        }
        yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
        yield "data: [DONE]\n\n"


def _format_node_output(node_name: str, state_update: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–èŠ‚ç‚¹è¾“å‡º

    Args:
        node_name: èŠ‚ç‚¹åç§°
        state_update: çŠ¶æ€æ›´æ–°

    Returns:
        æ ¼å¼åŒ–åçš„è¾“å‡ºæ–‡æœ¬
    """
    try:
        # è·¯ç”±èŠ‚ç‚¹
        if node_name == "router":
            agent_plan = state_update.get("agent_plan", [])
            if agent_plan:
                output = "\nğŸ”€ **è·¯ç”±å†³ç­–**\n\n"
                for i, plan in enumerate(agent_plan, 1):
                    agent_name = plan.get("agent", "")
                    task = plan.get("task", "")
                    output += f"{i}. **{agent_name}**: {task}\n"
                output += "\n"
                return output
            return ""

        # ReAct æ€è€ƒèŠ‚ç‚¹ - ä» next_action è¯»å–å½“å‰æ€è€ƒç»“æœ
        elif node_name == "react_think":
            next_action = state_update.get("next_action", {})
            if next_action:
                thought = next_action.get("thought", "")
                action_type = next_action.get("action_type", "")
                tool_name = next_action.get("tool_name", "")
                params = next_action.get("params", {})

                if thought:
                    # ä½¿ç”¨çº¯ Markdown æ ¼å¼ï¼Œé»˜è®¤å±•å¼€
                    output = "\n#### ğŸ¤” æ€è€ƒä¸­...\n\n"
                    output += f"```\n{thought}\n```\n\n"

                    # å¦‚æœæœ‰è¡ŒåŠ¨å†³ç­–ï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ¥
                    if action_type == "TOOL":
                        output += f"ğŸ”§ **å‡†å¤‡æ‰§è¡Œå·¥å…·**: `{tool_name}`\n"
                        if params:
                            output += f"**å‚æ•°**: `{json.dumps(params, ensure_ascii=False)}`\n"
                        output += "\n"
                    elif action_type == "FINISH":
                        output += "âœ… **å‡†å¤‡å®Œæˆä»»åŠ¡**\n\n"

                    return output
            return ""

        # ReAct è§‚å¯ŸèŠ‚ç‚¹
        elif node_name == "react_observe":
            execution_history = state_update.get("execution_history", [])
            if execution_history:
                last_record = execution_history[-1]
                observation = last_record.get("observation", "")
                action = last_record.get("action", {})

                if observation:
                    # è·å–å·¥å…·åç§°å’Œç±»å‹
                    tool_name = action.get("tool", "") if isinstance(action, dict) else ""
                    tool_type = get_tool_type(tool_name) if tool_name else "default"

                    # å°è¯•æå–ç»“æ„åŒ–æ‘˜è¦
                    summary = extract_result_summary(tool_name, observation) if tool_name else None

                    # ä½¿ç”¨çº¯ Markdown æ ¼å¼ï¼Œé»˜è®¤å±•å¼€
                    output = "\n#### ğŸ“Š è§‚å¯Ÿç»“æœ\n\n"

                    # å¦‚æœæœ‰æ‘˜è¦ï¼Œå…ˆæ˜¾ç¤ºæ‘˜è¦
                    if summary:
                        output += f"> ğŸ“Œ **æ‘˜è¦**: {summary}\n\n"

                    # æ™ºèƒ½æˆªæ–­è§‚å¯Ÿç»“æœ
                    observation_display = smart_truncate(observation, tool_type)

                    # ä½¿ç”¨ä»£ç å—åŒ…è£¹ï¼Œä¿æŒæ ¼å¼
                    output += f"```\n{observation_display}\n```\n\n"

                    return output
            return ""

        # æœ€ç»ˆç­”æ¡ˆèŠ‚ç‚¹
        elif node_name == "final_answer":
            final_answer = state_update.get("final_answer", "")
            if final_answer:
                return final_answer
            return ""

        # å…¶ä»–èŠ‚ç‚¹ï¼ˆä¾‹å¦‚ switch_agent_nodeï¼‰
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰ Agent åˆ‡æ¢ä¿¡æ¯
            current_agent_index = state_update.get("current_agent_index")
            agent_plan = state_update.get("agent_plan", [])

            if current_agent_index is not None and agent_plan:
                if current_agent_index < len(agent_plan):
                    current_plan = agent_plan[current_agent_index]
                    agent_name = current_plan.get("agent", "")
                    return f"\nğŸ”„ **åˆ‡æ¢åˆ° Agent**: {agent_name}\n\n"

            return ""

    except Exception as e:
        logger.error(f"æ ¼å¼åŒ–èŠ‚ç‚¹è¾“å‡ºå¤±è´¥ ({node_name}): {e}")
        return ""
